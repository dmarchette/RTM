\section{Open Archives}

<<nopref,echo=FALSE,results='hide'>>=
# A Prefix nulling hook.
# from http://stackoverflow.com/questions/22524822/how-can-i-remove-the-prefix-index-indicator-1-in-knitr-output

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_nullit <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_nullit <- isTRUE( knitr::opts_current$get("no_pref") )
  if( can_nullit && do_nullit ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})

knitr::opts_template$set("kill_pref"=list(comment=NA, no_pref=TRUE))
@

Open archives (\url{www.openarchives.org})
provide access to a huge number of document archives.
Check out \url{http://www.openarchives.org/Register/BrowseSites/}
for a list of registered data providers. 

One very nice repository for scientific article data is arXiv
(\url{arxiv.org}). Here's how to
use the OAIHarvester package to obtain a document set from arXiv.
First, we get a list of the available data sets.

<<qfin,cache=TRUE,tidy=FALSE>>=
library(OAIHarvester)
library(XML)
baseurl <- "http://export.arxiv.org/oai2"
x <- oaih_list_sets(baseurl)
x[,1:2]
@

Now we download the document meta data using \texttt{oaih\_list\_records}.
For a reasonable sized document set, consider the subject
``Quantitative Finance'' from the arxiv repository:

<<qfin2,cache=TRUE,echo=2:3,tidy=FALSE>>=
arqfin.date <- Sys.time()
x <- oaih_list_records(baseurl,set="q-fin")
arXivQFin <- oaih_transform(x[,"metadata"])
## If the above is run, we are changing the qfin
## data, so we need to ensure that it is reloaded.
if(file.exists('Data/arqfin.RData'))
   unlink('Data/arqfin.RData')
@

There are \Sexpr{nrow(arXivQFin)} documents (as 
of \Sexpr{format(arqfin.date,"%B %d, %Y")} -- 
the collection grows, of course, so you'll
get more when you run the above code).

The information returned for these documents is:
<<setoptsT1,results='hide',echo=FALSE>>=
w <- options("width")
options(width=60)
@

<<qfin3,cache=TRUE,tidy=FALSE>>=
colnames(arXivQFin)
@

Now, consider just those articles with the phrase
``Quantitative Finance'' in their
subjects:

<<qfin4,cache=TRUE,tidy=FALSE,echo=1:5>>=
a <- unlist(lapply(arXivQFin[,"subject"],function(x) 
                           length(grep("Quantitative Finance",x))))
b <- which(a==1)
qf <- arXivQFin[b,]
qf.primary <- unlist(lapply(qf[,"subject"],function(x) 
                          x[grep("Quantitative Finance",x)]))
qf.primary <- gsub("Quantitative Finance - ","",
                   qf.primary)
if(file.exists('Data/arqfin.RData')){
load('Data/arqfin.RData')
} else {
save(arXivQFin,arqfin.date,qf,qf.primary,file='Data/arqfin.RData')
}
@
<<qfin4table,cache=TRUE,tidy=FALSE,echo=1:5>>=
table(qf.primary)
@

This reduces the set of documents to \Sexpr{length(qf.primary)}, with
the above table of primary subjects.

<<unsetoptsT1,results='hide',echo=FALSE>>=
options(width=w$width)
@

A similar procedure obtains the abstracts from
the Journal of Statistical Software:
\label{jss.obtain}
<<jssdata1,tidy=FALSE>>=
library(tm)
library(OAIHarvester)
library(XML)

getData <- function()
{
   x <- oaih_list_records("http://www.jstatsoft.org/oai")
   JSS_papers <- oaih_transform(x[,"metadata"])
   ## order by time
   ord <- order(as.Date(unlist(JSS_papers[,"date"])))
   JSS_papers <- JSS_papers[ord,]
   ## only those papers with an abstract
   abst <- grep("Abstract:",JSS_papers[,"description"])
   JSS_papers <- JSS_papers[abst,]
   JSS_papers
}

remove_HTML_markup <- function(s)
{
   doc <- htmlTreeParse(s,asText=TRUE,trim=FALSE)
   xmlValue(xmlRoot(doc))
}
@

<<jssdata,cache=TRUE,echo=1,tidy=FALSE>>=
JSS_papers <- getData()

if(file.exists("Data/JSS_papers.RData")){
load("Data/JSS_papers.RData")
} else {
jss.date <- Sys.time()
save(JSS_papers,jss.date,file="Data/JSS_papers.RData")
}
@


\section{Searching PubMed}
There are many ways to get data from PubMed. One method uses
the \texttt{RISmed} package.
<<pmEbola,echo=1:3,tidy=FALSE,cache=TRUE>>=
library(RISmed)
ebolaSummary <- EUtilsSummary('ebola hemorrhagic',
   type='esearch',db='pubmed')
ebolaDocs <- EUtilsGet(ebolaSummary,
   type='efetch',db='pubmed')
if(file.exists("Data/ebola.RData")){
load("Data/ebola.RData")
} else {
ebola.date <- Sys.time()
save(ebolaDocs,ebolaSummary,ebola.date,file="Data/ebola.RData")
}
@
<<pmEbola2,echo=c(1:6),tidy=FALSE>>=
Author(ebolaDocs)[[1]]
ArticleTitle(ebolaDocs)[[1]]
@
\begin{figure}
<<pmEbolaFig,echo=FALSE,fig.width=4,fig.height=4,results='hide'>>=
y <- Year(ebolaDocs)
barplot(table(y),las=2,ylab="# Articles",cex.names=0.75)
@
\caption{\label{fig:ebolaPM}
Number of Ebola articles in PubMed per year.
Data collected on \Sexpr{format(ebola.date,"%B %d, %Y")}.
}
\end{figure}

The abstracts can be extracted using \texttt{AbstractText}.
To see what fields are accessible from the data, we can
list them:

<<ebolaList,echo=TRUE>>=
slotNames(ebolaDocs)
@

Note that there is a limit to the amount of data that can be
downloaded at one time. In order to avoid over taxing the server
(and risk being temporarily blocked) one should limit the
number of queries, and schedule larger requests for weekends
or nights (Eastern time).

\section{RSS feed examples: Google news and others}

Obtaining data from an RSS feed is simple using the \texttt{XML} package.

For example, consider the Ars Mathematica blog, 
\url{http://www.arsmathematica.net/}.
We can download the data from this feed using \texttt{xmlParse}:

<<ars,cache=TRUE,echo=1:4,tidy=FALSE>>=
  doc <- xmlParse("http://www.arsmathematica.net/?feed=rss2")
  titles <- xpathSApply(xmlRoot(doc),"//item/title",
                        xmlValue)
  descriptions <- xpathSApply(xmlRoot(doc),"//item/description",
                        xmlValue)
  contents <- xpathSApply(xmlRoot(doc),"//item/content:encoded",
                        xmlValue)
  if(file.exists('Data/arsmath.RData')){
  load('Data/arsmath.RData')
  } else {
  arsmath.date <- Sys.time()
  save(doc,titles,descriptions,contents,arsmath.date,file='Data/arsmath.RData')
  }
@

<<arsout,echo=3:4,tidy=FALSE,no_pref=TRUE>>=
  w <- options("width")
  options(width=60)
  strwrap(titles[[1]])
  strwrap(contents[[1]])[1:10]
  options(width=w$width)
@

Obtaining Google News, or any other RSS feed, is done the same way.
In this case we'll grab the Science news. This code is adapted from
code provided by Zach Mayer in his blog, which can be obtained at
\url{http://www.r-bloggers.com/importing-google-news-data-to-r/}.
Our version is simpler, but shows some basic ideas of
how to obtain and parse data from RSS feeds such as Google News.

<<googlenews,cache=TRUE,echo=1:20,tidy=FALSE>>=
library(XML)
library(plyr)
## Get at most 300 articles from the US Science News section of Google
## Note that 300 is a maximum set by Google News
gnURL <- paste(
  "http://news.google.com/",
  "news?pz=1&cf=all&num=300",
  "&ned=us&hl=en&topic=snc&output=rss",sep="")
doc <- xmlTreeParse(gnURL,useInternalNodes=TRUE)
nodes = getNodeSet(doc, "//item")
## Convert the data to a data frame
mydf = ldply(nodes, as.data.frame(xmlToList))
colnames(mydf) <- gsub("value\\.","",colnames(mydf))

## Function to convert the html in the description
## to text
parseDescription <- function(str) {
   out <- paste(unlist(
      xpathApply(htmlParse(str, asText=TRUE),
        "//body//text()", 
        xmlValue)),
	      collapse="\n")
   z <- strsplit(out,'\n')[[1]]
   outlet <- z[1]
   lead <- z[2]
   story <- paste(z[-(1:3)],collapse=" ")
   out <- data.frame(outlet=outlet,lead=lead,story=story)
   out
}
## Apply the function to the descriptions
## and wrap everything up into a data frame
description <- lapply(mydf$description,parseDescription)
description <- do.call(rbind,description)
docs <- cbind(mydf,description)

if(file.exists('Data/googleSciNews.RData')){
load('Data/googleSciNews.RData')
} else {
gsinews.date <- Sys.time()
save(docs,gscinews.date,file='Data/googleSciNews.RData')
}
@

As with all RSS feeds, one obtains the most recent 
articles. In order to obtain a larger collection, one must 
either set up a script to check the server periodically (and
remove duplicates) or find an archive of the data one is interested in,
and download that.


\section{Other examples}

Data can be obtained from the web using the \texttt{download.file}
function. This allows us to specify the URL of the file, and download it
into a local file. 

Here is an example using Project Gutenberg. First, let's get the
index, and look through it for something interesting:
<<gutenberg,cache=TRUE,tidy=FALSE>>=
download.file("http://www.gutenberg.org/dirs/GUTINDEX.ALL",
              destfile="Data/gutenberg.txt")
gt <- readLines("Data/gutenberg.txt")
gsub("\\s+"," ",
	gt[grep("Complete Works of William Shakespeare",gt)])
@

Now let's get the works of Shakespeare.\footnote{This
example is borrowed (with a few minor modifications) from 
\url{http://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/}.}
After a small amount of experimenting, we can determine the
filename, and download the file.

<<shakespeare,cache=TRUE,echo=c(2,4:22),tidy=FALSE>>=
if(!file.exists("Data/shakespeare.txt")){
download.file("http://www.gutenberg.org/files/100/100.txt", 
              destfile = "Data/shakespeare.txt")
}
## Read the data in and make a single character string
txt <- readLines("Data/shakespeare.txt")
dates <- grep("^1[56][[:digit:]][[:digit:]]",txt)
titles <- rep('',length(dates))
## The titles follow one to five blank lines
for(i in 1:length(dates)){
   for(j in 1:6){
      if(nchar(txt[dates[i]+j])>3){
         titles[i] <- txt[dates[i]+j]
         break
      }
   }
}
dates <- as.numeric(txt[dates])
txt <- paste(txt,collapse=' ')
## split on the "<<" marking sections
works <- unlist(strsplit(txt,"<<[^>]*>>"))
## remove the head and tail
shakespeare <- works[-c(1,2,length(works))]
@
Note that Project Gutenberg does not allow bulk downloads. Getting data
this way is fine for one document now and then, but do not try to obtain
dozens of books this way.

After the above processing, we have \Sexpr{length(shakespeare)} documents.
Now let's assign class labels to these documents
(each act becomes the same class as the associated play).

<<dpclass,tidy=FALSE,echo=1:2>>=
a <- diff(grep("1[56][[:digit:]]+",shakespeare))
shakespeare.class <- rep(1:length(titles),
                        times=c(a,1))
save(shakespeare,titles,dates,shakespeare.class,file="Data/shakespeare.RData")
@

Some simple text processing to look at the documents:

<<shakeprocess,cache=TRUE,tidy=FALSE>>=
library(proxy)

## get the words from each document
docs <- gsub("[[:punct:]]|\\d"," ",shakespeare)
words <- strsplit(tolower(docs),split=" ")

## remove 1 and 2 character words
words <- lapply(words,function(x) x[nchar(x)>2])

## define the lexicon
lexicon <- sort(unique(unlist(words)))

## count the words
n <- length(docs)
out <- matrix(0,nrow=n,ncol=length(lexicon))
for(i in 1:n){
   count <- table(words[[i]])
   out[i,which(lexicon %in% names(count))] <- count
}

## remove any words occuring in fewer than 3 documents
out <- out[,which(apply(out,2,function(x) sum(x>1))>3)]

D <- dist(out,method='cosine')
X <- cmdscale(D)
@

\begin{figure}
<<shakepict,echo=FALSE,results='hide'>>=
plot(X,type='n',xlab=expression(X[1]),ylab=expression(X[2]))
text(X,labels=shakespeare.class,cex=.75)
@
\caption{\label{fig:shakespeare}
Plot of the Complete Words of Shakespeare. 
The document labeled $1$ contains the sonnets, each other
number represents
an act from one of the plays, with each play numbered consecutively
(as they appear in the original file).
}
\end{figure}

\section{Data in R Packages}
Several R packages have text data. This section is by no means comprehensive,
but will point out some of the resources that are readily available. 

<<tm,echo=FALSE,results='hide'>>=
library(tm)
data(acq)
data(crude)
@
The \texttt{tm} package has two smaller corpora from Reuters: \texttt{acq},
a set of \Sexpr{length(acq)}
articles on corporate acquisitions, and \texttt{crude},
a set of \Sexpr{length(crude)} articles about crude oil.

The \texttt{languageR} package contains the books {\it Moby Dick}
and {\it Alice's Adventures in Wonderland}. We can split these into
corpora where each document corresponds to a chapter of the book.
For example, for Moby Dick, the following code extracts the chapters.

<<moby,echo=TRUE>>=
library(languageR)
M <- paste(moby,collapse=" ")
chapters <- gregexpr("CHAPTER [[:digit:]]+",M)[[1]][-33]
docs <- rep("",length(chapters))
for(i in 2:length(chapters)){
   a <- chapters[i-1]
   b <- chapters[i]-1
   docs[i-1] <- substring(M,a,b)
}
a <- chapters[length(chapters)]
docs[length(chapters)] <- substring(M,a,nchar(M))
substring(docs[1],1,34)
substring(docs[135],1,31)
@

Similarly, for Alice:
<<alice,echo=TRUE>>=
A <- paste(alice,collapse=" ")
chapters <- gregexpr("CHAPTER",A)[[1]]
docs <- rep("",length(chapters))
for(i in 2:length(chapters)){
   a <- chapters[i-1]
   b <- chapters[i]-1
   docs[i-1] <- substring(A,a,b)
}
a <- chapters[length(chapters)]
docs[length(chapters)] <- substring(A,a,nchar(A))
substring(docs[1],1,30)
@

<<maxent,echo=FALSE,results='hide'>>=
library(maxent)
suppressWarnings({
NYTimes <- read.csv(system.file("data/NYTimes.csv.gz",package="maxent"),
       stringsAsFactors=FALSE)
USCongress <- read.csv(system.file("data/USCongress.csv.gz",package="maxent"),
       stringsAsFactors=FALSE)
})
@
The \texttt{maxent} package contains a set of 
\Sexpr{nrow(NYTimes)} labeled headlines from the New York Times.
These have \Sexpr{length(unique(NYTimes[,'Topic.Code']))}
topic codes associated with them, but these are not described in the
help page associated with the data. The package also contains a set
of \Sexpr{nrow(USCongress)} bills from the US Congress. The
text corresponds to a short description of the bill.

There are other packages which contain processed data, such as word
counts. For example, the \texttt{textir} package contains phrase counts
for a set of restaurant reviews, with ratings, and phrase counts and
ideology scores for members of the $109$th United States Congress.
